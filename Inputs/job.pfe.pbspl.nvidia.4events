#!/bin/csh
#PBS -S /bin/csh
#PBS -N SWPCsky
#PBS -q v100@pbspl4

# This script should be started from the directory
# above the Event01 ... directories.
# Runs up to 4 events on 4 GPUs on cas_gpu or sky_gpu.
# Can be used with qsub.pfe.pbspl.pl if desired.

# Monitoring performance:
# qstat -n -u $USER
# ssh COMPUTENODE
# nvidia-smi -l 1 # monitor GPUs
# top             # monitor CPUs

# To run on v100 GPU (up to 4)
#PBS -lselect=1:ncpus=36:ngpus=4:model=sky_gpu
###PBS -lselect=1:ncpus=48:ngpus=4:model=cas_gpu

# Set maximum walltime
#PBS -l walltime=24:00:00

# Combine STDOUT and STDERR
#PBS -j oe

# Send email if something happens
#PBS -m abe

# Specify group (account charged) if necessary
### PBS -W group_list=...

# Load nvidia runtime libraries (should be the same as used for compilation)
module purge
module use -a /nasa/nvidia/hpc_sdk/modulefiles
module load nvhpc/20.9
# module load nvhpc/21.2

echo `which nvfortran`
echo `which mpiexec`

# cd into the Runs/run1 directory
cd $PBS_O_WORKDIR

# This is running on many nodes with CPUs
#echo "PBS_NOFEFILE=$PBS_NODEFILE"
#cat $PBS_NODEFILE > hosts
#head -2 hosts           > Event01/hosts
#tail -6 | head -2 hosts > Event02/hosts
#tail -4 | head -2 hosts > Event03/hosts
#tail -2                 > Event04/hosts
#setenv PBS_NODEFILE ./hosts

setenv MPI_DSM_VERBOSE
setenv OMPI_MCA_mpi_warn_on_fork 0

# dplace is needed to run on the cores near the selected GPU. 
# -s1 is skipping the shepherd process, -c ... selects the cores for the 2 MPI processes.
# GPU0 and GPU1 should use the first half of cores, GPU2 and GPU3 the second half

# Run 4 events
(cd Event01; setenv ACC_DEVICE_NUM 0; mpiexec -n 2 dplace -s1 -c 1,2   -o dplace.log ./SWMF.exe > runlog.`date +%y%m%d`) &
(cd Event02; setenv ACC_DEVICE_NUM 1; mpiexec -n 2 dplace -s1 -c 3,4   -o dplace.log ./SWMF.exe > runlog.`date +%y%m%d`) &
(cd Event03; setenv ACC_DEVICE_NUM 2; mpiexec -n 2 dplace -s1 -c 25,26 -o dplace.log ./SWMF.exe > runlog.`date +%y%m%d`) &
(cd Event04; setenv ACC_DEVICE_NUM 3; mpiexec -n 2 dplace -s1 -c 27,28 -o dplace.log ./SWMF.exe > runlog.`date +%y%m%d`) &
wait

# resubmission is not handled yet
exit 0

# Use the #CPUTIMEMAX and #CHECKSTOP commands in PARAM.in
# so the code stops before the wall clock time is exceeded.

# Do not continue unless the job finished successfully
if(! -f SWMF.SUCCESS) exit

# Do not continue if the whole run is done
if(-f SWMF.DONE) exit

# Link latest restart files
./Restart.pl

# Provide a PARAM.in.restart file if you wish and uncomment these lines:
if(! -f PARAM.in.start) cp PARAM.in PARAM.in.start
if(-f PARAM.in.restart) cp PARAM.in.restart PARAM.in

# Resubmit job
qsub job.pfe.pbspl.nvidia
